{"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"colab":{"provenance":[],"gpuType":"V28"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12494290,"sourceType":"datasetVersion","datasetId":7721113}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"papermill":{"default_parameters":{},"duration":15047.222344,"end_time":"2025-07-19T16:04:23.626580","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-07-19T11:53:36.404236","version":"2.6.0"},"accelerator":"TPU"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from huggingface_hub import HfApi , hf_hub_download\nimport os\nimport tqdm\n\n\napi = HfApi()\nrepo_type = \"dataset\"","metadata":{"id":"1z8KGDQtGTaJ"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This Notebook was used to transfer files from huggingface to kaggle as the data processing and training was conducted on kaggle\nDue to offline nature (disabled internet access) for the L4 GPUs, it was necessary to perform the data processing before hand and not on-the-fly during training to save GPU time and to save memory related errors \n","metadata":{}},{"cell_type":"code","source":"i = 4\nrepo_id = f\"aneeshm44/reg{i}\"\nall_files = api.list_repo_files(repo_id=repo_id, repo_type=repo_type)\ntiff_files = [file for file in all_files if file.endswith(\".tiff\")]\nlen(tiff_files)","metadata":{"id":"8Dx19LK4GQ48","outputId":"1eace438-ae02-4524-dce4-94dbdc619f06"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"start , end = 500 , 1020\n\nos.makedirs(\"/content/files\", exist_ok=True)\n\nfor i in tqdm.tqdm(range(start, min(end, len(tiff_files))) , leave=False):\n    file_path = tiff_files[i]\n    print(f\"Downloading {i+1}/{min(end, len(tiff_files))}: {file_path}\")\n\n    try:\n        downloaded_path = hf_hub_download(\n            repo_id=repo_id,\n            repo_type=repo_type,\n            filename=file_path,\n            local_dir=\"/content/files\",\n            local_dir_use_symlinks=False  ,\n        )\n        print(f\"Downloaded to: {downloaded_path}\")\n    except Exception as e:\n        print(f\"Error downloading {file_path}: {e}\")","metadata":{"id":"Go_-FqJsGNnB"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(os.listdir(\"/content/files\"))","metadata":{"id":"3ovJoSbPK14o"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"users =  \"aneeshmukkamala\" \nkeys =\n\nindex = 0\nos.environ[\"KAGGLE_USERNAME\"] = user\nos.environ[\"KAGGLE_KEY\"] = key\n\n\nfrom kaggle.api.kaggle_api_extended import KaggleApi\nimport os\nimport json\n\nSAVE_FOLDER =\"/content/files\"\napi = KaggleApi()\napi.authenticate()\napi.dataset_initialize(SAVE_FOLDER)\n\npart = 11\nDATASET_NAME = f\"REGHF{part}\"\nusername = users[index]\n\nwith open(f\"{SAVE_FOLDER}/dataset-metadata.json\", \"r\") as f:\n    dataset_meta = json.load(f)\n\ndataset_meta[\"id\"] = f\"{username}/{DATASET_NAME}\"\ndataset_meta[\"title\"] = DATASET_NAME\ndataset_meta[\"licenses\"] = [{\"name\": \"CC0-1.0\"}]\nwith open(f\"{SAVE_FOLDER}/dataset-metadata.json\", \"w\") as outfile:\n    json.dump(dataset_meta, outfile, indent=4)\n\napi.dataset_create_new(SAVE_FOLDER ,  public=True)\nprint(f\"Dataset created successfully for {username}!\")","metadata":{"id":"xvN5gi5nGeud"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"f4_DnuMMLBIt"},"outputs":[],"execution_count":null}]}
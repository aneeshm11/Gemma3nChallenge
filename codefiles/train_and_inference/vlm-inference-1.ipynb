{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PACKAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-08-03T01:41:43.981442Z",
     "iopub.status.busy": "2025-08-03T01:41:43.980995Z",
     "iopub.status.idle": "2025-08-03T01:41:49.751386Z",
     "shell.execute_reply": "2025-08-03T01:41:49.750763Z",
     "shell.execute_reply.started": "2025-08-03T01:41:43.981418Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip uninstall timm transformers accelerate peft -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-08-03T01:41:49.752766Z",
     "iopub.status.busy": "2025-08-03T01:41:49.752554Z",
     "iopub.status.idle": "2025-08-03T01:42:19.771288Z",
     "shell.execute_reply": "2025-08-03T01:42:19.770726Z",
     "shell.execute_reply.started": "2025-08-03T01:41:49.752745Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"/kaggle/input/pip-vlmfs\")\n",
    "\n",
    "import json  , math , timm , einops\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np \n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer \n",
    "\n",
    "import lightning as LIGHTNING\n",
    "from lightning.pytorch.callbacks import Callback\n",
    "\n",
    "from typing import List, Union , Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LANGUAGE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-03T01:42:19.772233Z",
     "iopub.status.busy": "2025-08-03T01:42:19.771894Z",
     "iopub.status.idle": "2025-08-03T01:43:55.872400Z",
     "shell.execute_reply": "2025-08-03T01:43:55.871818Z",
     "shell.execute_reply.started": "2025-08-03T01:42:19.772217Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_path = \"/kaggle/input/vlm-merger\"\n",
    "language_model = AutoModelForCausalLM.from_pretrained(\n",
    "    # MODEL_PATH,\n",
    "    model_path , \n",
    "    trust_remote_code=True,\n",
    "    device_map=\"cuda:0\",\n",
    "    torch_dtype = torch.bfloat16,\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VISION MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-03T01:43:55.874263Z",
     "iopub.status.busy": "2025-08-03T01:43:55.873721Z",
     "iopub.status.idle": "2025-08-03T01:43:56.489794Z",
     "shell.execute_reply": "2025-08-03T01:43:56.489205Z",
     "shell.execute_reply.started": "2025-08-03T01:43:55.874244Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TimmCNNModel(nn.Module):\n",
    "    def __init__(self, num_classes: int = 8, model_name: str = \"mobilenetv4_conv_medium.e500_r256_in1k\"):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.backbone = timm.create_model(\n",
    "             'local-dir:/kaggle/input/codefiles/efficientnet_b0/efficientnet_b0',\n",
    "            pretrained=True,\n",
    "            num_classes=0,\n",
    "        )\n",
    "        \n",
    "        self.feature_dim = self.backbone.num_features\n",
    "        \n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(self.feature_dim, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm1d(512),\n",
    "        \n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    \n",
    "    def forward_features(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.backbone(x)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        features = self.forward_features(x)\n",
    "        logits = self.classifier(features)\n",
    "        return logits\n",
    "\n",
    "image_model = TimmCNNModel(num_classes=8)\n",
    "\n",
    "weights = torch.load(\"/kaggle/input/d/aneeshmukkamala/timmweights/finalcheckpoint.pth\")\n",
    "image_model.load_state_dict(weights['model_state_dict'])\n",
    "\n",
    "for param in image_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROJECTOR MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-03T01:43:56.490644Z",
     "iopub.status.busy": "2025-08-03T01:43:56.490430Z",
     "iopub.status.idle": "2025-08-03T01:43:59.874270Z",
     "shell.execute_reply": "2025-08-03T01:43:59.873638Z",
     "shell.execute_reply.started": "2025-08-03T01:43:56.490627Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Projector_4to3d(nn.Module):\n",
    "        \n",
    "    def __init__(self, cnn_dim: int = 1280, llm_dim: int = 2048, num_heads: int = 8, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.cnn_dim = cnn_dim\n",
    "        self.llm_dim = llm_dim\n",
    "        \n",
    "        # Spatial positional embeddings for 8x8 grid\n",
    "        self.spatial_pos_embed = nn.Parameter(torch.randn(64, cnn_dim))\n",
    "        \n",
    "        # Multi-scale feature processing\n",
    "        self.spatial_conv = nn.Conv2d(cnn_dim, cnn_dim // 2, 1)  # Reduce channels while preserving spatial\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d(1)  # Global context\n",
    "        \n",
    "        # Enhanced projection layers\n",
    "        self.input_proj = nn.Sequential(\n",
    "            nn.Linear(cnn_dim, llm_dim),\n",
    "            nn.LayerNorm(llm_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        # Multi-head self-attention for spatial reasoning\n",
    "        self.spatial_attention = nn.MultiheadAttention(\n",
    "            embed_dim=llm_dim,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Cross-attention for text-image alignment\n",
    "        self.cross_attention = nn.MultiheadAttention(\n",
    "            embed_dim=llm_dim,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(llm_dim)\n",
    "        self.norm2 = nn.LayerNorm(llm_dim)\n",
    "        \n",
    "        # Enhanced FFN\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(llm_dim, llm_dim * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(llm_dim * 4, llm_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        self.norm3 = nn.LayerNorm(llm_dim)\n",
    "        \n",
    "        # Token compression layer (optional - reduces from 64 to fewer tokens)\n",
    "        self.compress_tokens = nn.Parameter(torch.randn(32, llm_dim))  # Learnable query tokens\n",
    "        self.token_compression = nn.MultiheadAttention(\n",
    "            embed_dim=llm_dim,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "            elif isinstance(module, nn.LayerNorm):\n",
    "                nn.init.ones_(module.weight)\n",
    "                nn.init.zeros_(module.bias)\n",
    "            elif isinstance(module, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(module.weight)\n",
    "    \n",
    "    def forward(self, cnn_features: torch.Tensor, text_embeddings: torch.Tensor = None) -> torch.Tensor:\n",
    "        batch_size = cnn_features.shape[0]\n",
    "        \n",
    "        # Multi-scale processing\n",
    "        spatial_features = self.spatial_conv(cnn_features)  # (B, 640, 8, 8)\n",
    "        global_context = self.global_pool(cnn_features).flatten(1)  # (B, 1280)\n",
    "        \n",
    "        # Flatten spatial features and add positional encoding\n",
    "        x = einops.rearrange(cnn_features, \"b c h w -> b (h w) c\")  # (B, 64, 1280)\n",
    "        pos_embeddings = self.spatial_pos_embed.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "        x = x + pos_embeddings\n",
    "        \n",
    "        # Project to LLM dimension - keep in float32 for attention operations\n",
    "        x = self.input_proj(x)  # (B, 64, 2048)\n",
    "        \n",
    "        # Self-attention for spatial reasoning\n",
    "        attended_x, spatial_attn_weights = self.spatial_attention(x, x, x)\n",
    "        x = self.norm1(x + attended_x)\n",
    "        \n",
    "        # Cross-attention with text (if available during training)\n",
    "        if text_embeddings is not None:\n",
    "            # Convert text embeddings to float32 for attention computation\n",
    "            text_embeddings_float = text_embeddings.float()\n",
    "            cross_attended, cross_attn_weights = self.cross_attention(x, text_embeddings_float, text_embeddings_float)\n",
    "            x = self.norm2(x + cross_attended)\n",
    "        \n",
    "        # FFN\n",
    "        ffn_out = self.ffn(x)\n",
    "        x = self.norm3(x + ffn_out)\n",
    "        \n",
    "        # Optional token compression\n",
    "        compress_queries = self.compress_tokens.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "        compressed_x, _ = self.token_compression(compress_queries, x, x)\n",
    "        \n",
    "        return compressed_x  # (B, 32, 2048) - compressed representation\n",
    "\n",
    "\n",
    "projector = Projector_4to3d(cnn_dim=1280, llm_dim=2048, num_heads=8)\n",
    "checkpoint = \"/kaggle/input/lmweights/vmweights/projector.pth\"\n",
    "weights = torch.load(checkpoint)\n",
    "projector.load_state_dict(weights)\n",
    "\n",
    "for param in projector.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# weights.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIGHTNING MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-03T01:43:59.875250Z",
     "iopub.status.busy": "2025-08-03T01:43:59.875023Z",
     "iopub.status.idle": "2025-08-03T01:43:59.891412Z",
     "shell.execute_reply": "2025-08-03T01:43:59.890860Z",
     "shell.execute_reply.started": "2025-08-03T01:43:59.875233Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, image_model, language_model, projector, tokenizer, prompt=\"Describe the medical image:\"):\n",
    "        super().__init__()\n",
    "        self.image_model = image_model \n",
    "        self.language_model = language_model\n",
    "        self.projector = projector\n",
    "        self.tokenizer = tokenizer\n",
    "        self.eos_token = tokenizer.eos_token\n",
    "        self.prompt = prompt\n",
    "        \n",
    "        device = next(self.language_model.parameters()).device\n",
    "        \n",
    "        self.image_model.to(device)\n",
    "        self.projector.to(device)\n",
    "        \n",
    "        # Create prompt embeddings\n",
    "        prompt_tokens = tokenizer(text=prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "        prompt_embeddings = language_model.get_input_embeddings()(prompt_tokens).detach()\n",
    "        self.register_buffer('prompt_embeddings', prompt_embeddings)\n",
    "        \n",
    "        # Contrastive learning components\n",
    "        self.temperature = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n",
    "        self.image_projection_head = nn.Sequential(\n",
    "            nn.Linear(2048, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256)\n",
    "        ).to(device)\n",
    "        self.text_projection_head = nn.Sequential(\n",
    "            nn.Linear(2048, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256)\n",
    "        ).to(device)\n",
    "    \n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "    \n",
    "    def forward(self, patches: torch.Tensor, texts: List[str], compute_contrastive: bool = True):\n",
    "        device = self.device\n",
    "        patches = patches.to(device)\n",
    "        \n",
    "        image_features = self.image_model.backbone.forward_features(patches)\n",
    "        \n",
    "        tokenized = self.tokenizer(\n",
    "            text=[text + self.tokenizer.eos_token for text in texts],\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=CFG.MAX_LENGTH,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        tokenized = {k: v.to(device) for k, v in tokenized.items()}\n",
    "        text_embeddings = self.language_model.get_input_embeddings()(tokenized[\"input_ids\"])\n",
    "        \n",
    "        patch_embeddings = self.projector(image_features, text_embeddings)\n",
    "        \n",
    "        patch_embeddings = patch_embeddings.to(torch.bfloat16)\n",
    "        text_embeddings = text_embeddings.to(torch.bfloat16)\n",
    "        \n",
    "        # Concatenate embeddings\n",
    "        embeddings = torch.cat([\n",
    "            self.prompt_embeddings.expand(patches.size(0), -1, -1),\n",
    "            patch_embeddings,\n",
    "            text_embeddings,\n",
    "        ], dim=1)\n",
    "        \n",
    "        # Create attention mask\n",
    "        prompt_mask = torch.ones(patches.size(0), self.prompt_embeddings.size(1), device=device)\n",
    "        patch_mask = torch.ones(patches.size(0), patch_embeddings.size(1), device=device)\n",
    "        attention_mask = torch.cat([prompt_mask, patch_mask, tokenized[\"attention_mask\"]], dim=1)\n",
    "        \n",
    "        # Create labels\n",
    "        prompt_labels = torch.full((patches.size(0), self.prompt_embeddings.size(1)), CFG.LABEL_MASK, device=device)\n",
    "        patch_labels = torch.full((patches.size(0), patch_embeddings.size(1)), CFG.LABEL_MASK, device=device)\n",
    "        text_labels = tokenized[\"input_ids\"].clone()\n",
    "        labels = torch.cat([prompt_labels, patch_labels, text_labels], dim=1)\n",
    "        labels[attention_mask == 0] = CFG.LABEL_MASK\n",
    "        \n",
    "        llm_output = self.language_model(\n",
    "            inputs_embeds=embeddings,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        \n",
    "        total_loss = llm_output.loss\n",
    "        loss_dict = {\"language_loss\": llm_output.loss}\n",
    "        \n",
    "        if compute_contrastive:\n",
    "            # Contrastive loss between image and text\n",
    "            image_global = patch_embeddings.mean(dim=1)  # Global image representation\n",
    "            text_global = text_embeddings.mean(dim=1)    # Global text representation\n",
    "            \n",
    "            # Project to contrastive space\n",
    "            image_proj = self.image_projection_head(image_global.float())\n",
    "            text_proj = self.text_projection_head(text_global.float())\n",
    "            \n",
    "            # Normalize\n",
    "            image_proj = F.normalize(image_proj, dim=-1)\n",
    "            text_proj = F.normalize(text_proj, dim=-1)\n",
    "            \n",
    "            # Compute contrastive loss\n",
    "            logits = torch.matmul(image_proj, text_proj.t()) * self.temperature.exp()\n",
    "            labels_contrastive = torch.arange(len(logits), device=device)\n",
    "            \n",
    "            contrastive_loss = (F.cross_entropy(logits, labels_contrastive) + \n",
    "                              F.cross_entropy(logits.t(), labels_contrastive)) / 2\n",
    "            \n",
    "            total_loss = total_loss + 0.1 * contrastive_loss  \n",
    "            loss_dict[\"contrastive_loss\"] = contrastive_loss\n",
    "        \n",
    "        # Attention regularization loss\n",
    "        if hasattr(self.projector, 'spatial_attention'):\n",
    "            # Encourage attention diversity (prevent attention collapse)\n",
    "            attn_entropy_loss = 0.0\n",
    "            # This would be computed from attention weights if we save them\n",
    "            loss_dict[\"attention_entropy_loss\"] = attn_entropy_loss\n",
    "        \n",
    "        return {\n",
    "            \"loss\": total_loss,\n",
    "            \"logits\": llm_output.logits,\n",
    "            \"loss_breakdown\": loss_dict\n",
    "        }\n",
    "    \n",
    "    def generate(self, patches: torch.Tensor, generator_kwargs: dict[str, Union[int, float]]):\n",
    "        device = self.device\n",
    "        patches = patches.to(device)\n",
    "        \n",
    "        image_features = self.image_model.backbone.forward_features(patches)\n",
    "        patch_embeddings = self.projector(image_features)\n",
    "        patch_embeddings = patch_embeddings.to(torch.bfloat16)\n",
    "        \n",
    "        embeddings = torch.cat([\n",
    "            self.prompt_embeddings.expand(patches.size(0), -1, -1),\n",
    "            patch_embeddings,\n",
    "        ], dim=1)\n",
    "        \n",
    "        prompt_mask = torch.ones(patches.size(0), self.prompt_embeddings.size(1), device=device)\n",
    "        patch_mask = torch.ones(patches.size(0), patch_embeddings.size(1), device=device)\n",
    "        attention_mask = torch.cat([prompt_mask, patch_mask], dim=1)\n",
    "        \n",
    "        return self.language_model.generate(\n",
    "            inputs_embeds=embeddings,\n",
    "            attention_mask=attention_mask,\n",
    "            **generator_kwargs\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-03T01:43:59.892137Z",
     "iopub.status.busy": "2025-08-03T01:43:59.891961Z",
     "iopub.status.idle": "2025-08-03T01:44:00.182363Z",
     "shell.execute_reply": "2025-08-03T01:44:00.181771Z",
     "shell.execute_reply.started": "2025-08-03T01:43:59.892122Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "vlm_model = Model(image_model, language_model, projector , tokenizer, prompt=\"Describe this image:\")\n",
    "vlm_model = vlm_model.to(torch.device(\"cuda:0\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-03T01:44:00.183196Z",
     "iopub.status.busy": "2025-08-03T01:44:00.183002Z",
     "iopub.status.idle": "2025-08-03T01:44:00.323818Z",
     "shell.execute_reply": "2025-08-03T01:44:00.323211Z",
     "shell.execute_reply.started": "2025-08-03T01:44:00.183180Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "patches = np.load(\"/kaggle/input/miccaireg/images.npy\"  , mmap_mode = \"r\")\n",
    "\n",
    "start = 5100\n",
    "end = 5109\n",
    "patches_batch = np.array(patches[start:end])\n",
    "patches_batch = torch.tensor(patches_batch)\n",
    "patches_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-03T01:44:00.324616Z",
     "iopub.status.busy": "2025-08-03T01:44:00.324407Z",
     "iopub.status.idle": "2025-08-03T01:46:06.900637Z",
     "shell.execute_reply": "2025-08-03T01:46:06.900080Z",
     "shell.execute_reply.started": "2025-08-03T01:44:00.324599Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "generator_kwargs = {\n",
    "    \n",
    "    \"max_new_tokens\": 50,\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 0.4,\n",
    "    \"top_p\": 0.9,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id\n",
    "}\n",
    "    \n",
    "batch_size = 3\n",
    "generated_ids = []\n",
    "\n",
    "for i in tqdm(range(0, len(patches_batch), batch_size)):\n",
    "    batch_chunk = patches_batch[i:i+batch_size]\n",
    "    chunk_ids = vlm_model.generate(batch_chunk, generator_kwargs)\n",
    "    generated_ids.extend(chunk_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-03T01:47:24.718494Z",
     "iopub.status.busy": "2025-08-03T01:47:24.717848Z",
     "iopub.status.idle": "2025-08-03T01:47:24.723377Z",
     "shell.execute_reply": "2025-08-03T01:47:24.722871Z",
     "shell.execute_reply.started": "2025-08-03T01:47:24.718453Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "generated_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-03T01:48:48.761213Z",
     "iopub.status.busy": "2025-08-03T01:48:48.760715Z",
     "iopub.status.idle": "2025-08-03T01:48:48.765770Z",
     "shell.execute_reply": "2025-08-03T01:48:48.765250Z",
     "shell.execute_reply.started": "2025-08-03T01:48:48.761191Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "generated_texts = []\n",
    "for new_tokens in generated_ids:\n",
    "    text = tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
    "    generated_texts.append(text.strip())\n",
    "\n",
    "with open(\"inference.json\" ,\"w\") as f:\n",
    "    json.dump(generated_texts , f , indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# with open(\"/kaggle/input/codefiles/train.json\" , \"r\") as f:\n",
    "#     d = json.load(f)\n",
    "\n",
    "# d[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-03T01:49:19.722294Z",
     "iopub.status.busy": "2025-08-03T01:49:19.721649Z",
     "iopub.status.idle": "2025-08-03T01:49:19.725740Z",
     "shell.execute_reply": "2025-08-03T01:49:19.725244Z",
     "shell.execute_reply.started": "2025-08-03T01:49:19.722272Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "generated_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-08-03T01:46:06.908129Z",
     "iopub.status.idle": "2025-08-03T01:46:06.908358Z",
     "shell.execute_reply": "2025-08-03T01:46:06.908260Z",
     "shell.execute_reply.started": "2025-08-03T01:46:06.908251Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaL4",
   "dataSources": [
    {
     "databundleVersionId": 11802066,
     "isSourceIdPinned": false,
     "sourceId": 91496,
     "sourceType": "competition"
    },
    {
     "datasetId": 7956444,
     "sourceId": 12597068,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7866202,
     "sourceId": 12647806,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7992932,
     "isSourceIdPinned": true,
     "sourceId": 12647979,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7721113,
     "sourceId": 12649060,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7995534,
     "isSourceIdPinned": true,
     "sourceId": 12651951,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 251516524,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 253278745,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 253869955,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

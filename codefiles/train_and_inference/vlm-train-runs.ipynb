{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":91496,"databundleVersionId":11802066,"sourceType":"competition"},{"sourceId":12486891,"sourceType":"datasetVersion","datasetId":7879509},{"sourceId":12647806,"sourceType":"datasetVersion","datasetId":7866202},{"sourceId":12647979,"sourceType":"datasetVersion","datasetId":7992932},{"sourceId":12649060,"sourceType":"datasetVersion","datasetId":7721113},{"sourceId":251516524,"sourceType":"kernelVersion"},{"sourceId":253278745,"sourceType":"kernelVersion"}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip uninstall transformers timm accelerate peft unsloth bitsandbytes xformers -y","metadata":{"execution":{"iopub.status.busy":"2025-08-02T18:31:36.285647Z","iopub.execute_input":"2025-08-02T18:31:36.285959Z","iopub.status.idle":"2025-08-02T18:31:42.647709Z","shell.execute_reply.started":"2025-08-02T18:31:36.285941Z","shell.execute_reply":"2025-08-02T18:31:42.647097Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%capture\nimport os\n\nos.environ[\"WANDB_DISABLED\"] = \"true\"\nos.environ[\"WANDB_MODE\"] = \"disabled\"","metadata":{"execution":{"iopub.status.busy":"2025-08-02T18:31:42.648675Z","iopub.execute_input":"2025-08-02T18:31:42.648910Z","iopub.status.idle":"2025-08-02T18:31:42.653269Z","shell.execute_reply.started":"2025-08-02T18:31:42.648880Z","shell.execute_reply":"2025-08-02T18:31:42.652769Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%capture\n\nimport sys\n\nsys.path.append(\"/kaggle/input/pip-unsloth\")\nsys.path.append(\"/kaggle/input/pip-vlmfs\")\n# sys.path.append(\"/kaggle/input/pip-quantization\")\n\nimport unsloth\nfrom unsloth import FastModel\nimport json  , math , timm , einops\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np \n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer \n\nimport lightning as LIGHTNING\nfrom lightning.pytorch.callbacks import Callback\n\nfrom typing import List, Union , Dict","metadata":{"execution":{"iopub.status.busy":"2025-08-02T18:31:42.654456Z","iopub.execute_input":"2025-08-02T18:31:42.654781Z","iopub.status.idle":"2025-08-02T18:32:50.375585Z","shell.execute_reply.started":"2025-08-02T18:31:42.654765Z","shell.execute_reply":"2025-08-02T18:32:50.375006Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model, tokenizer = FastModel.from_pretrained(\n    model_name = \"/kaggle/input/gemma3\",\n    dtype = torch.bfloat16, \n    # dtype = torch.float32, \n\n    load_in_4bit = False , \n    full_finetuning = False,\n)","metadata":{"execution":{"iopub.status.busy":"2025-08-02T18:32:50.376178Z","iopub.execute_input":"2025-08-02T18:32:50.376712Z","iopub.status.idle":"2025-08-02T18:38:02.794917Z","shell.execute_reply.started":"2025-08-02T18:32:50.376693Z","shell.execute_reply":"2025-08-02T18:38:02.794349Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"language_model = FastModel.get_peft_model(\n    model,\n    finetune_vision_layers     = False, \n    finetune_audio_layers      = False,\n    \n    finetune_language_layers   = True, \n    finetune_attention_modules = True,  \n    finetune_mlp_modules       = True,  \n\n    r = 256,\n    lora_alpha = 256*4,\n    lora_dropout = 0.04,\n    bias = \"none\",\n)","metadata":{"execution":{"iopub.status.busy":"2025-08-02T18:38:02.795565Z","iopub.execute_input":"2025-08-02T18:38:02.795756Z","iopub.status.idle":"2025-08-02T18:38:11.590580Z","shell.execute_reply.started":"2025-08-02T18:38:02.795741Z","shell.execute_reply":"2025-08-02T18:38:11.589972Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"language_model = language_model.to(dtype=torch.bfloat16)\nlanguage_model.print_trainable_parameters()","metadata":{"execution":{"iopub.status.busy":"2025-08-02T18:38:11.591250Z","iopub.execute_input":"2025-08-02T18:38:11.591671Z","iopub.status.idle":"2025-08-02T18:38:11.653310Z","shell.execute_reply.started":"2025-08-02T18:38:11.591654Z","shell.execute_reply":"2025-08-02T18:38:11.652797Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# IMAGE COMPONENTS","metadata":{}},{"cell_type":"markdown","source":"### IMAGE MODEL","metadata":{}},{"cell_type":"code","source":"class TimmCNNModel(nn.Module):\n    # timm/mobilenetv4_conv_medium.e500_r256_in1k\n    def __init__(self, num_classes: int = 8, model_name: str = \"efficientnet_b0\"):\n        super().__init__()\n        \n        self.backbone = timm.create_model(\n             'local-dir:/kaggle/input/codefiles/efficientnet_b0/efficientnet_b0',\n            pretrained=True,\n            num_classes=0,\n            )\n        \n        self.feature_dim = self.backbone.num_features\n        \n\n        self.classifier = nn.Sequential(\n            nn.Dropout(0.1),\n            nn.Linear(self.feature_dim, 512),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm1d(512),\n        \n            nn.Dropout(0.1),\n            nn.Linear(512, 256),\n            nn.ReLU(inplace=True),\n            nn.Linear(256, num_classes)\n        )\n\n        \n    def forward_features(self, x: torch.Tensor) -> torch.Tensor:\n        return self.backbone(x)\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        features = self.forward_features(x)\n        logits = self.classifier(features)\n        return logits\n\n\nimage_model = TimmCNNModel(num_classes=8)\n\nweights = torch.load(\"/kaggle/input/timmweights/finalcheckpoint.pth\")\nimage_model.load_state_dict(weights['model_state_dict'])\n\nfor param in image_model.parameters():\n    param.requires_grad = False","metadata":{"execution":{"iopub.status.busy":"2025-08-02T18:38:11.653931Z","iopub.execute_input":"2025-08-02T18:38:11.654137Z","iopub.status.idle":"2025-08-02T18:38:12.376097Z","shell.execute_reply.started":"2025-08-02T18:38:11.654123Z","shell.execute_reply":"2025-08-02T18:38:12.375523Z"},"scrolled":true,"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_model.feature_dim","metadata":{"execution":{"iopub.status.busy":"2025-08-02T18:38:12.376730Z","iopub.execute_input":"2025-08-02T18:38:12.376929Z","iopub.status.idle":"2025-08-02T18:38:12.380468Z","shell.execute_reply.started":"2025-08-02T18:38:12.376913Z","shell.execute_reply":"2025-08-02T18:38:12.379980Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### PROJECTOR","metadata":{}},{"cell_type":"code","source":"class Projector_4to3d(nn.Module):\n        \n    def __init__(self, cnn_dim: int = 1280, llm_dim: int = 2048, num_heads: int = 8, dropout: float = 0.1):\n        super().__init__()\n        self.cnn_dim = cnn_dim\n        self.llm_dim = llm_dim\n        \n        # Spatial positional embeddings for 8x8 grid\n        self.spatial_pos_embed = nn.Parameter(torch.randn(64, cnn_dim))\n        \n        # Multi-scale feature processing\n        self.spatial_conv = nn.Conv2d(cnn_dim, cnn_dim // 2, 1)  # Reduce channels while preserving spatial\n        self.global_pool = nn.AdaptiveAvgPool2d(1)  # Global context\n        \n        # Enhanced projection layers\n        self.input_proj = nn.Sequential(\n            nn.Linear(cnn_dim, llm_dim),\n            nn.LayerNorm(llm_dim),\n            nn.ReLU(),\n            nn.Dropout(dropout)\n        )\n        \n        # Multi-head self-attention for spatial reasoning\n        self.spatial_attention = nn.MultiheadAttention(\n            embed_dim=llm_dim,\n            num_heads=num_heads,\n            dropout=dropout,\n            batch_first=True\n        )\n        \n        # Cross-attention for text-image alignment\n        self.cross_attention = nn.MultiheadAttention(\n            embed_dim=llm_dim,\n            num_heads=num_heads,\n            dropout=dropout,\n            batch_first=True\n        )\n        \n        self.norm1 = nn.LayerNorm(llm_dim)\n        self.norm2 = nn.LayerNorm(llm_dim)\n        \n        # Enhanced FFN\n        self.ffn = nn.Sequential(\n            nn.Linear(llm_dim, llm_dim * 4),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(llm_dim * 4, llm_dim),\n            nn.Dropout(dropout)\n        )\n        \n        self.norm3 = nn.LayerNorm(llm_dim)\n        \n        # Token compression layer (optional - reduces from 64 to fewer tokens)\n        self.compress_tokens = nn.Parameter(torch.randn(32, llm_dim))  # Learnable query tokens\n        self.token_compression = nn.MultiheadAttention(\n            embed_dim=llm_dim,\n            num_heads=num_heads,\n            dropout=dropout,\n            batch_first=True\n        )\n        \n        self._init_weights()\n    \n    def _init_weights(self):\n        for module in self.modules():\n            if isinstance(module, nn.Linear):\n                nn.init.xavier_uniform_(module.weight)\n                if module.bias is not None:\n                    nn.init.zeros_(module.bias)\n            elif isinstance(module, nn.LayerNorm):\n                nn.init.ones_(module.weight)\n                nn.init.zeros_(module.bias)\n            elif isinstance(module, nn.Conv2d):\n                nn.init.kaiming_normal_(module.weight)\n    \n    def forward(self, cnn_features: torch.Tensor, text_embeddings: torch.Tensor = None) -> torch.Tensor:\n        batch_size = cnn_features.shape[0]\n        \n        # Multi-scale processing\n        spatial_features = self.spatial_conv(cnn_features)  # (B, 640, 8, 8)\n        global_context = self.global_pool(cnn_features).flatten(1)  # (B, 1280)\n        \n        # Flatten spatial features and add positional encoding\n        x = einops.rearrange(cnn_features, \"b c h w -> b (h w) c\")  # (B, 64, 1280)\n        pos_embeddings = self.spatial_pos_embed.unsqueeze(0).expand(batch_size, -1, -1)\n        x = x + pos_embeddings\n        \n        # Project to LLM dimension - keep in float32 for attention operations\n        x = self.input_proj(x)  # (B, 64, 2048)\n        \n        # Self-attention for spatial reasoning\n        attended_x, spatial_attn_weights = self.spatial_attention(x, x, x)\n        x = self.norm1(x + attended_x)\n        \n        # Cross-attention with text (if available during training)\n        if text_embeddings is not None:\n            # Convert text embeddings to float32 for attention computation\n            text_embeddings_float = text_embeddings.float()\n            cross_attended, cross_attn_weights = self.cross_attention(x, text_embeddings_float, text_embeddings_float)\n            x = self.norm2(x + cross_attended)\n        \n        # FFN\n        ffn_out = self.ffn(x)\n        x = self.norm3(x + ffn_out)\n        \n        # Optional token compression\n        compress_queries = self.compress_tokens.unsqueeze(0).expand(batch_size, -1, -1)\n        compressed_x, _ = self.token_compression(compress_queries, x, x)\n        \n        return compressed_x  # (B, 32, 2048) - compressed representation\n\n\nprojector = Projector_4to3d(cnn_dim=1280, llm_dim=2048, num_heads=8)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2025-08-02T18:38:12.382348Z","iopub.execute_input":"2025-08-02T18:38:12.382523Z","iopub.status.idle":"2025-08-02T18:38:13.402593Z","shell.execute_reply.started":"2025-08-02T18:38:12.382509Z","shell.execute_reply":"2025-08-02T18:38:13.402003Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"lora weights are in float32 always","metadata":{}},{"cell_type":"markdown","source":"# DATALOADER","metadata":{}},{"cell_type":"code","source":"class CFG:\n    \n    EPOCHS = 18\n    GRAD_ACC = 1\n    \n    TRAIN_BATCH_SIZE = 10\n    \n    TRAIN_START = 0\n    TRAIN_END = 5000\n    \n    LABEL_MASK = -100\n    MAX_LENGTH = 50\n    \n    VM_LR = 2e-4\n    LLM_LR = 2e-5\n","metadata":{"execution":{"iopub.status.busy":"2025-08-02T18:38:13.403220Z","iopub.execute_input":"2025-08-02T18:38:13.403417Z","iopub.status.idle":"2025-08-02T18:38:13.406598Z","shell.execute_reply.started":"2025-08-02T18:38:13.403402Z","shell.execute_reply":"2025-08-02T18:38:13.406120Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with open(\"/kaggle/input/miccaireg/labels.json\", \"r\") as f:\n    texts = json.load(f)\n\npatches = np.load(\"/kaggle/input/miccaireg/images.npy\" , mmap_mode=\"r\")\n\nprint(f\"Patches shape: {np.shape(patches)}, Texts length: {len(texts)}\")","metadata":{"execution":{"iopub.status.busy":"2025-08-02T18:38:13.407143Z","iopub.execute_input":"2025-08-02T18:38:13.407309Z","iopub.status.idle":"2025-08-02T18:38:13.453990Z","shell.execute_reply.started":"2025-08-02T18:38:13.407296Z","shell.execute_reply":"2025-08-02T18:38:13.453438Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"l = []\n\nfor x in texts:\n    l.append(len(tokenizer(text=x)[\"input_ids\"][0]))\n\nimport matplotlib.pyplot as plt \nplt.plot(l)","metadata":{"execution":{"iopub.status.busy":"2025-08-02T18:38:13.454567Z","iopub.execute_input":"2025-08-02T18:38:13.454955Z","iopub.status.idle":"2025-08-02T18:38:14.615210Z","shell.execute_reply.started":"2025-08-02T18:38:13.454938Z","shell.execute_reply":"2025-08-02T18:38:14.614709Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"c = 0\n\nfor x in l:\n    if x<50:\n        c+=1\n\nc","metadata":{"execution":{"iopub.status.busy":"2025-08-02T18:38:14.615832Z","iopub.execute_input":"2025-08-02T18:38:14.616035Z","iopub.status.idle":"2025-08-02T18:38:14.620272Z","shell.execute_reply.started":"2025-08-02T18:38:14.616021Z","shell.execute_reply":"2025-08-02T18:38:14.619793Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class REGDataset(Dataset):\n    def __init__(self, patches_mmap, texts: list[str], start_idx: int, end_idx: int):\n        self.patches_mmap = patches_mmap\n        self.texts = texts\n        self.start_idx = start_idx\n        self.end_idx = end_idx\n    \n    def __len__(self):\n        return self.end_idx - self.start_idx\n    \n    def __getitem__(self, idx):\n        actual_idx = self.start_idx + idx\n        patches = (self.patches_mmap[actual_idx])\n        texts = self.texts[actual_idx]\n        return torch.tensor(patches), texts\n\n\ntrain_data = REGDataset(patches, texts, CFG.TRAIN_START, CFG.TRAIN_END)\ntrain_dl = DataLoader(train_data, CFG.TRAIN_BATCH_SIZE, pin_memory=True, shuffle=True, drop_last=True)","metadata":{"execution":{"iopub.status.busy":"2025-08-02T18:38:14.620814Z","iopub.execute_input":"2025-08-02T18:38:14.620998Z","iopub.status.idle":"2025-08-02T18:38:14.629207Z","shell.execute_reply.started":"2025-08-02T18:38:14.620985Z","shell.execute_reply":"2025-08-02T18:38:14.628735Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# MODEL","metadata":{}},{"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self, image_model, language_model, projector, tokenizer, prompt=\"Describe the medical image:\"):\n        super().__init__()\n        self.image_model = image_model \n        self.language_model = language_model\n        self.projector = projector\n        self.tokenizer = tokenizer\n        self.eos_token = tokenizer.eos_token\n        self.prompt = prompt\n        \n        device = next(self.language_model.parameters()).device\n        \n        self.image_model.to(device)\n        self.projector.to(device)\n        \n        # Create prompt embeddings\n        prompt_tokens = tokenizer(text=prompt, return_tensors=\"pt\").input_ids.to(device)\n        prompt_embeddings = language_model.get_input_embeddings()(prompt_tokens).detach()\n        self.register_buffer('prompt_embeddings', prompt_embeddings)\n        \n        # Contrastive learning components\n        self.temperature = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n        self.image_projection_head = nn.Sequential(\n            nn.Linear(2048, 512),\n            nn.ReLU(),\n            nn.Linear(512, 256)\n        ).to(device)\n        self.text_projection_head = nn.Sequential(\n            nn.Linear(2048, 512),\n            nn.ReLU(),\n            nn.Linear(512, 256)\n        ).to(device)\n    \n    @property\n    def device(self):\n        return next(self.parameters()).device\n    \n    def forward(self, patches: torch.Tensor, texts: List[str], compute_contrastive: bool = True):\n        device = self.device\n        patches = patches.to(device)\n        \n        image_features = self.image_model.backbone.forward_features(patches)\n        \n        tokenized = self.tokenizer(\n            text=[text + self.tokenizer.eos_token for text in texts],\n            padding=True,\n            truncation=True,\n            max_length=CFG.MAX_LENGTH,\n            return_tensors=\"pt\",\n        )\n        tokenized = {k: v.to(device) for k, v in tokenized.items()}\n        text_embeddings = self.language_model.get_input_embeddings()(tokenized[\"input_ids\"])\n        \n        patch_embeddings = self.projector(image_features, text_embeddings)\n        \n        patch_embeddings = patch_embeddings.to(torch.bfloat16)\n        text_embeddings = text_embeddings.to(torch.bfloat16)\n        \n        # Concatenate embeddings\n        embeddings = torch.cat([\n            self.prompt_embeddings.expand(patches.size(0), -1, -1),\n            patch_embeddings,\n            text_embeddings,\n        ], dim=1)\n        \n        # Create attention mask\n        prompt_mask = torch.ones(patches.size(0), self.prompt_embeddings.size(1), device=device)\n        patch_mask = torch.ones(patches.size(0), patch_embeddings.size(1), device=device)\n        attention_mask = torch.cat([prompt_mask, patch_mask, tokenized[\"attention_mask\"]], dim=1)\n        \n        # Create labels\n        prompt_labels = torch.full((patches.size(0), self.prompt_embeddings.size(1)), CFG.LABEL_MASK, device=device)\n        patch_labels = torch.full((patches.size(0), patch_embeddings.size(1)), CFG.LABEL_MASK, device=device)\n        text_labels = tokenized[\"input_ids\"].clone()\n        labels = torch.cat([prompt_labels, patch_labels, text_labels], dim=1)\n        labels[attention_mask == 0] = CFG.LABEL_MASK\n        \n        llm_output = self.language_model(\n            inputs_embeds=embeddings,\n            attention_mask=attention_mask,\n            labels=labels\n        )\n        \n        total_loss = llm_output.loss\n        loss_dict = {\"language_loss\": llm_output.loss}\n        \n        if compute_contrastive:\n            # Contrastive loss between image and text\n            image_global = patch_embeddings.mean(dim=1)  # Global image representation\n            text_global = text_embeddings.mean(dim=1)    # Global text representation\n            \n            # Project to contrastive space\n            image_proj = self.image_projection_head(image_global.float())\n            text_proj = self.text_projection_head(text_global.float())\n            \n            # Normalize\n            image_proj = F.normalize(image_proj, dim=-1)\n            text_proj = F.normalize(text_proj, dim=-1)\n            \n            # Compute contrastive loss\n            logits = torch.matmul(image_proj, text_proj.t()) * self.temperature.exp()\n            labels_contrastive = torch.arange(len(logits), device=device)\n            \n            contrastive_loss = (F.cross_entropy(logits, labels_contrastive) + \n                              F.cross_entropy(logits.t(), labels_contrastive)) / 2\n            \n            total_loss = total_loss + 0.1 * contrastive_loss  \n            loss_dict[\"contrastive_loss\"] = contrastive_loss\n        \n        # Attention regularization loss\n        if hasattr(self.projector, 'spatial_attention'):\n            # Encourage attention diversity (prevent attention collapse)\n            attn_entropy_loss = 0.0\n            # This would be computed from attention weights if we save them\n            loss_dict[\"attention_entropy_loss\"] = attn_entropy_loss\n        \n        return {\n            \"loss\": total_loss,\n            \"logits\": llm_output.logits,\n            \"loss_breakdown\": loss_dict\n        }\n    \n    def generate(self, patches: torch.Tensor, generator_kwargs: dict[str, Union[int, float]]):\n        device = self.device\n        patches = patches.to(device)\n        \n        image_features = self.image_model.backbone.forward_features(patches)\n        patch_embeddings = self.projector(image_features)\n        patch_embeddings = patch_embeddings.to(torch.bfloat16)\n        \n        embeddings = torch.cat([\n            self.prompt_embeddings.expand(patches.size(0), -1, -1),\n            patch_embeddings,\n        ], dim=1)\n        \n        prompt_mask = torch.ones(patches.size(0), self.prompt_embeddings.size(1), device=device)\n        patch_mask = torch.ones(patches.size(0), patch_embeddings.size(1), device=device)\n        attention_mask = torch.cat([prompt_mask, patch_mask], dim=1)\n        \n        return self.language_model.generate(\n            inputs_embeds=embeddings,\n            attention_mask=attention_mask,\n            **generator_kwargs\n        )","metadata":{"execution":{"iopub.status.busy":"2025-08-02T18:38:14.629703Z","iopub.execute_input":"2025-08-02T18:38:14.629859Z","iopub.status.idle":"2025-08-02T18:38:14.645788Z","shell.execute_reply.started":"2025-08-02T18:38:14.629846Z","shell.execute_reply":"2025-08-02T18:38:14.645321Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class LightningModule(LIGHTNING.LightningModule):\n    def __init__(self, model: Model):\n        super().__init__()\n        self.model = model\n        self.automatic_optimization = False  \n    \n    def training_step(self, batch, batch_idx):\n        opt = self.optimizers()\n        sch = self.lr_schedulers()\n        \n        patches, texts = batch\n        \n        output = self.model(patches, texts, compute_contrastive=True)\n        \n        total_loss = output[\"loss\"]\n        loss_breakdown = output[\"loss_breakdown\"]\n        \n        self.manual_backward(total_loss)\n        \n        self.clip_gradients(opt, gradient_clip_val=1.0, gradient_clip_algorithm=\"norm\")\n        \n        opt.step()\n        opt.zero_grad()\n        sch.step()\n        \n        self.log(\"train_loss\", total_loss, prog_bar=True)\n        for loss_name, loss_value in loss_breakdown.items():\n            self.log(f\"{loss_name}\", loss_value)\n        \n        \n        return total_loss        \n    def configure_optimizers(self):\n        params = [\n            {\"params\": self.model.projector.parameters(), \"lr\": CFG.VM_LR, \"weight_decay\": 1e-4},\n            {\"params\": self.model.image_projection_head.parameters(), \"lr\": CFG.VM_LR, \"weight_decay\": 1e-4},\n            {\"params\": self.model.text_projection_head.parameters(), \"lr\": CFG.VM_LR, \"weight_decay\": 1e-4},\n            {\"params\": [self.model.temperature], \"lr\": CFG.VM_LR, \"weight_decay\": 0.0},\n            {\"params\": [p for p in self.model.language_model.parameters() if p.requires_grad], \"lr\": CFG.LLM_LR, \"weight_decay\": 1e-5}\n        ]\n\n        optimizer = torch.optim.AdamW(params, eps=1e-8)\n        \n        # Cosine annealing with warmup\n        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n            optimizer,\n            max_lr=[param_group[\"lr\"] for param_group in optimizer.param_groups],\n            total_steps=self.trainer.estimated_stepping_batches,\n            pct_start=0.1,  # 10% warmup\n            anneal_strategy='cos',\n            div_factor=25.0,\n            final_div_factor=1000.0,\n        )\n        \n        return [optimizer], [{\"scheduler\": scheduler, \"interval\": \"step\"}]\n","metadata":{"execution":{"iopub.status.busy":"2025-08-02T18:38:14.646340Z","iopub.execute_input":"2025-08-02T18:38:14.646505Z","iopub.status.idle":"2025-08-02T18:38:14.659518Z","shell.execute_reply.started":"2025-08-02T18:38:14.646492Z","shell.execute_reply":"2025-08-02T18:38:14.659083Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = Model(image_model, language_model, projector , tokenizer)\nlightning_module = LightningModule(model)","metadata":{"execution":{"iopub.status.busy":"2025-08-02T18:38:14.660044Z","iopub.execute_input":"2025-08-02T18:38:14.660214Z","iopub.status.idle":"2025-08-02T18:38:14.920331Z","shell.execute_reply.started":"2025-08-02T18:38:14.660200Z","shell.execute_reply":"2025-08-02T18:38:14.919761Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"step_train_losses = []\nstep_language_losses = []\nstep_contrastive_losses = []\nstep_attention_entropy_losses = []\n\nclass PrintLossCallback(Callback):\n    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):\n        metrics = trainer.callback_metrics\n        \n        if 'train_loss' in metrics:\n            step_train_losses.append(float(metrics['train_loss']))\n        if 'train_language_loss' in metrics:\n            step_language_losses.append(float(metrics['train_language_loss']))\n        if 'train_contrastive_loss' in metrics:\n            step_contrastive_losses.append(float(metrics['train_contrastive_loss']))\n        if 'train_attention_entropy_loss' in metrics:\n            step_attention_entropy_losses.append(float(metrics['train_attention_entropy_loss']))\n\n    def on_train_epoch_end(self, trainer, pl_module):\n        metrics = trainer.callback_metrics\n        text = f\"\\nEpoch {trainer.current_epoch} Summary:\"\n        for key, value in metrics.items():\n            text += f\"  {key}: {value:.4f}\"\n                \n        print(text)","metadata":{"execution":{"iopub.status.busy":"2025-08-02T18:59:35.391403Z","iopub.execute_input":"2025-08-02T18:59:35.392033Z","iopub.status.idle":"2025-08-02T18:59:35.397436Z","shell.execute_reply.started":"2025-08-02T18:59:35.392011Z","shell.execute_reply":"2025-08-02T18:59:35.396883Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer = LIGHTNING.Trainer(\n    max_epochs=  CFG.EPOCHS,\n    # precision='bf16',\n    # precision = \"32\",\n    accumulate_grad_batches= CFG.GRAD_ACC,\n    # gradient_clip_val=1.0,\n    enable_progress_bar=True,\n    \n    log_every_n_steps=100,\n    enable_checkpointing=False,\n    callbacks=[PrintLossCallback()]\n)","metadata":{"execution":{"iopub.status.busy":"2025-08-02T18:59:35.604360Z","iopub.execute_input":"2025-08-02T18:59:35.604945Z","iopub.status.idle":"2025-08-02T18:59:35.670515Z","shell.execute_reply.started":"2025-08-02T18:59:35.604924Z","shell.execute_reply":"2025-08-02T18:59:35.669970Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"l = []\nfor c in language_model.parameters():\n    l.append(c.dtype)\n\nset(l)","metadata":{"execution":{"iopub.status.busy":"2025-08-02T18:59:36.000538Z","iopub.execute_input":"2025-08-02T18:59:36.001149Z","iopub.status.idle":"2025-08-02T18:59:36.016275Z","shell.execute_reply.started":"2025-08-02T18:59:36.001126Z","shell.execute_reply":"2025-08-02T18:59:36.015768Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer.fit(\n    model = lightning_module,\n    train_dataloaders = train_dl,\n    datamodule = None,\n)","metadata":{"execution":{"iopub.status.busy":"2025-08-02T18:59:36.187423Z","iopub.execute_input":"2025-08-02T18:59:36.187953Z","execution_failed":"2025-08-02T19:05:56.751Z"},"scrolled":true,"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"os.makedirs(\"vmweights\", exist_ok=True)\nos.makedirs(\"lmweights_lora\", exist_ok=True)\n\nlightning_module.model.language_model.save_pretrained(\"/kaggle/working/lmweights_lora/\")\ntorch.save(lightning_module.model.projector.state_dict(), \"/kaggle/working/vmweights/projector.pth\")","metadata":{"execution":{"iopub.status.busy":"2025-08-02T18:47:29.140997Z","iopub.status.idle":"2025-08-02T18:47:29.141474Z","shell.execute_reply.started":"2025-08-02T18:47:29.141342Z","shell.execute_reply":"2025-08-02T18:47:29.141354Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"try:\n    np.save('step_train_losses.npy', np.array(step_train_losses))\n    np.save('step_language_losses.npy', np.array(step_language_losses))\n    np.save('step_contrastive_losses.npy', np.array(step_contrastive_losses))\n    np.save('step_attention_entropy_losses.npy', np.array(step_attention_entropy_losses))\nexcept Exception as e:\n    print(e)","metadata":{"execution":{"iopub.status.busy":"2025-08-02T18:47:29.142094Z","iopub.status.idle":"2025-08-02T18:47:29.142356Z","shell.execute_reply.started":"2025-08-02T18:47:29.142249Z","shell.execute_reply":"2025-08-02T18:47:29.142259Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}